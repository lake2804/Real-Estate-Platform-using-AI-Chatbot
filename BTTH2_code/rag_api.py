# rag_api.py
import time
import json
import faiss
import numpy as np
import requests
from fastapi import FastAPI, Request
from pydantic import BaseModel
from sentence_transformers import SentenceTransformer

# Load BGE M3 model
print("üîÅ Loading BGE-M3 model...")
model = SentenceTransformer("BAAI/bge-m3")
embedding_dim = model.get_sentence_embedding_dimension()

# Load FAISS index & metadata
print("üîÅ Loading FAISS index and metadata...")
index = faiss.read_index("../BTTH2_code/faiss_index/index.faiss")
with open("../BTTH2_code/faiss_index/metadata.json", "r", encoding="utf-8") as f:
    metadata = json.load(f)

# FastAPI init
app = FastAPI()

# Pydantic schema cho input
class QueryRequest(BaseModel):
    question: str
    top_k: int = 3  # s·ªë k·∫øt qu·∫£ tr·∫£ v·ªÅ, m·∫∑c ƒë·ªãnh l√† 3

def input_gen(text: str, question: str) -> str:
    return (
        f"D∆∞·ªõi ƒë√¢y l√† m·ªôt ƒëo·∫°n vƒÉn b·∫£n lu·∫≠t ƒë∆∞·ª£c tr√≠ch ra t·ª´ t√†i li·ªáu:\n{text}\n"
        f"C√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng:\n'{question}'\n"
        "D·ª±a tr√™n n·ªôi dung trong ƒëo·∫°n vƒÉn b·∫£n ·ªü tr√™n, h√£y ƒë√≥ng vai m·ªôt chuy√™n gia ph√°p l√Ω, "
        "am hi·ªÉu s√¢u v·ªÅ Lu·∫≠t ƒê·∫•t ƒëai Vi·ªát Nam nƒÉm 2013. Tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng d·ª±a tr√™n ƒëo·∫°n lu·∫≠t ƒë∆∞·ª£c cung c·∫•p, "
        "ƒë·∫£m b·∫£o ƒë√∫ng ng·ªØ c·∫£nh, ng√¥n ng·ªØ ph√°p l√Ω r√µ r√†ng, d·ªÖ hi·ªÉu. Y√™u c·∫ßu tr√≠ch d·∫´n ph·∫£i bao g·ªìm theo 'lu·∫≠t ƒë·∫•t ƒëai 2013', ch∆∞∆°ng v√† ƒëi·ªÅu. "
        "N·∫øu kh√¥ng t√¨m th·∫•y th√¥ng tin ph√π h·ª£p, h√£y tr·∫£ l·ªùi 'Kh√¥ng t√¨m th·∫•y th√¥ng tin trong ƒëo·∫°n tr√≠ch.'"
    )

def call_ollama(prompt: str, model: str = "mistral", max_tokens: int = 200) -> str:
    response = requests.post(
        "http://localhost:11434/api/generate",
        json={
            "model": model,
            "prompt": prompt,
            "stream": False,
            "options": {"num_predict": max_tokens}
        }
    )
    return response.json()["response"]

@app.post("/rag")
def rag_query(req: QueryRequest):
    start = time.time()

    # Encode c√¢u h·ªèi (chu·∫©n c√∫ ph√°p BGE)
    query_text = f"query: {req.question}"
    query_embedding = model.encode(query_text, normalize_embeddings=True)
    query_embedding = np.array([query_embedding], dtype="float32")

    # Search FAISS
    D, I = index.search(query_embedding, req.top_k)
    results = []

    text = ""
    for i in I[0]:
        if i < len(metadata):
            m = metadata[i]
            results.append({
                "chuong": m.get("chuong", ""),
                "tieu_de": m.get("tieu_de", ""),
                "noi_dung": m.get("noi_dung", "")
            })
            text += m.get("chuong", "")
            text += m.get("tieu_de", "")
            text += m.get("noi_dung", "")
            text += "\n"

    # G·ªçi LLM local ƒë·ªÉ sinh c√¢u tr·∫£ l·ªùi
    t_llm = time.time()
    answer = call_ollama(input_gen(text, req.question))
    llm_time = round(time.time() - t_llm, 3)

    return {
        "question": req.question,
        "top_k_chunks": results,
        "answer": answer,
        "response_time": round(time.time() - start, 3),
        "llm_time": llm_time
    }
